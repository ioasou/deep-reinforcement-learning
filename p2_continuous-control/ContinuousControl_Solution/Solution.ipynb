{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control Project Solution\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** _Ioannis Souflas_\n",
    "\n",
    "\n",
    "**Description:**\n",
    "The objective of this project is to train a Deep Deterministic Policy Gradient (DDPG) agent to control robotic arms using Unity's [Reacher]((https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher)) enviroment to maintain in contact with the green spheres.\n",
    "\n",
    "**External References:** [DDPG Algorithm - DeepMind](https://arxiv.org/pdf/1509.02971.pdf) \n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please that you have installed [Unity ML-Agents](https://pypi.org/project/unityagents/), [NumPy](http://www.numpy.org/), [Matplotlib](https://matplotlib.org/), [Collections](https://docs.python.org/2/library/collections.html), [Random](https://docs.python.org/2/library/random.html), [Time](https://docs.python.org/2/library/time.html) and [PyTorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# ddpg algorithm\n",
    "from ddpg_agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```\n",
    "\n",
    "The current configuration is **Linux** (x86_64, headless)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"../../p2_continuous-control/Reacher_Linux_NoVis/Reacher.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The shape of the state space looks like:', states.shape)\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this enviroment consists of 20 agents, as a results there are 20 different state observations,~ one for each agent. However, it is important to clarify that the actions is common for all agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. DDPG Agent Training Pipeline\n",
    "\n",
    "The `ddpg` function is making use of the `Agent` class in order to train the agent. Note that the following configuration can be used for both the single and multi (20) agent enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=500, max_t=1000, print_every=1, desired_reward=30.0, moving_average_lentgh=100, train_mode=True):\n",
    "    \n",
    "    episode_mean_scores = []                               \n",
    "    scores_buffer = deque(maxlen=moving_average_lentgh)  \n",
    "    moving_average = []                              \n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name] \n",
    "        states = env_info.vector_observations                         \n",
    "        scores = np.zeros(num_agents)                           \n",
    "        agent.reset()\n",
    "        start_time = time.time()\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states, add_noise=True)         \n",
    "            env_info = env.step(actions)[brain_name]            \n",
    "            next_states = env_info.vector_observations          \n",
    "            rewards = env_info.rewards                          \n",
    "            dones = env_info.local_done                         \n",
    "            # save experience from each agent to replay buffer\n",
    "            for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "                agent.step(state, action, reward, next_state, done, t)             \n",
    "            states = next_states\n",
    "            scores += rewards        \n",
    "            if np.any(dones):                                   \n",
    "                break\n",
    "   \n",
    "        episode_mean_scores.append(np.mean(scores))           \n",
    "        scores_buffer.append(episode_mean_scores[-1])         \n",
    "        moving_average.append(np.mean(scores_buffer))    \n",
    "                \n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tMean over an Episode: {:.1f}\\tMoving Average: {:.1f}'.format(i_episode, episode_mean_scores[-1], moving_average[-1]))\n",
    "                         \n",
    "        if moving_average[-1] >= desired_reward and i_episode >= moving_average_lentgh and train_mode==True:\n",
    "            print('\\nAgent trained successfully!\\tMoving Average ={:.1f} over the last {} episodes'.format(\\\n",
    "                                    moving_average[-1], moving_average_lentgh))            \n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            break\n",
    "            \n",
    "    return episode_mean_scores, moving_average\n",
    "\n",
    "# run the training loop\n",
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=1)\n",
    "episodic_mean, moving_average = ddpg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot of Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episodic_mean, label='Episodic Mean')\n",
    "plt.plot(moving_average, label='Moving Average')\n",
    "plt.legend()\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
